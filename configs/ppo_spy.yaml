# PPO Configuration for SPY Trading

# Experiment
exp_name: "ppo_spy_base"
seed: 42

# Data
ticker: "SPY"
start_date: "2010-01-01"
end_date: "2020-01-01"  # Pre-COVID only to avoid distribution shift
data_dir: "data"
add_indicators: false

# Data splits
train_pct: 0.7
val_pct: 0.15
test_pct: 0.15

# Environment
window_size: 30
initial_balance: 10000.0
shares_per_trade: 10
transaction_cost_pct: 0.0  # Removed costs for initial learning
max_shares: 1000
normalize: true
features: ["open", "high", "low", "close", "volume"]  # Full OHLCV data

# PPO Agent - Balanced for normalized rewards
lr: 0.0001  # Increase from 0.00005 - can learn faster now with normalized rewards
gamma: 0.99
gae_lambda: 0.95
clip_epsilon: 0.2
value_coef: 0.5  # Increase from 0.25 - value loss is now manageable
entropy_coef: 0.01  # Reduce from 0.03 - less randomness
max_grad_norm: 0.5
hidden_dims: [128, 64]  # Increase from [64, 32] - need capacity for 153-dim state
dropout: 0.1  # Reduce from 0.2 - less aggressive

# Training
n_episodes: 500
ppo_epochs: 6  # Moderate number of updates
batch_size: 128  # Keep larger batches
val_frequency: 10
early_stopping_patience: 30
use_cuda: true
